---
title: "Results (Draft)"
author: '2466057'
date: "2024-06-03"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    css: "style.css"
    df_print: "paged"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = TRUE, 
                      fig.width = 10, 
                      fig.height = 8,
                      dpi = 120, 
                      fig.path = "./figures/result_draft/", 
                      cache.path = "./cache/result_draft/",
                      comment = "")

```

## Overview

---

Trained 7 machine learning models and one dummy classifier. The latter always predicts the most frequent class. This could provide some meaningful comparisons, for example if we plotted the difference between a model and the dummy. Although it is removed for most graphs for now. 

The seven models, and their 2/3 letter codes are shown below. 

```{r library and functions}
#| message = FALSE, 
#| warning = FALSE, 
#| error = FALSE 

library(tidyverse)
library(here)

## script to load in functions and variables (including data)
source(here("r/setup_feature.R"))

## define a custom ggplot theme - set as default
theme_set(report_theme(base_size = 10))

## all of the models generated

for (name in names(all_names)){
  cat(paste0(name, " - ", all_names[name], "\n"))
}

```



```{r}
## merge the metric dataframes together
metrics_df <- merge_metric_dfs()

## summarise the metrics by the model_code, i.e. across their models for each of K-folds
metric_code <- generate_rel_metrics(metric_df, "model_code")

## summarise the metrics by the moodel_id, i.e across the K-folds (preserving different model hyperparameters)
metric_id <- generate_rel_metrics(metric_df, "model_id") |> 
  mutate(model_code = str_extract(model_id, "[A-Z]+"))
```

The data consisted of all the hypertensive subtypes (excluding the healthy control), and not all the microRNA variables were used. First, they were filtered via Recursive Feature Elimination (RFE), where the base model was a Gradient Boosted Tree. This left 131 features, although the RFE could be improved by using an ensemble of various models, as we run the risk of over-fitting the Gradient Boosted Trees. For example, when we used Random Forest for RFE, it selected only 87 features. This is not an insignificant difference and should be explored in a bit more detail (for a later date though).  

Six of the seven types of models (not Gaussian Naive Bayes) were trained across a wide range of hyperparameters to obtain a collect of models of varying quality. Each model was evaluated with a battery of metrics which can be compared in two main groups. Firstly, we can look at the **metric type**, like precision, recall, F1-score or accuracy. Secondly, we could look at how it generalises to the multiclass problem, either through Micro, Macro or Weighted averaging of the values from a confusion matrix, i.e the **averaging method** . Some metrics natively handle the multiclass setting, like accuracy, Matthew Correlation Coefficient (MCC) and Cohen Kappa. Here, these are described to have an averaging method of "None", although "NA" might be more clear. More precisely, most metrics are formed as a combination of an *averaging method* and a *metric type*, like Macro Precision, Micro F1-score. 

Micro-averaging, which sums the False Positives, True Positive (TP), etc, across each class then performs the calculation. For recall, all the True positives are summed and then divided by the sum of True Positives and False Negatives (FN), ie TP / (TP + FN). In comparison, Macro-averaging calculates the metric for each class, then obtains the means. Concretely, Macro-Recall would find the recall for each class first, then find the mean. 

## Comparison of Metrics relative to the Mean {.tabset}

---

In order to compare the metrics to each other across the various models, the mean score of each model was found then obtained metric was divided by this mean. Consequently, a relative value of 1 indicates that metric is identical to the mean while values less than one indicate it is typically smaller than the mean (likely suggesting it is more conservative). Values greater than one can be interpreted as the opposite, indicating a more optimistic metric in general.

On the X-axis we have all of model type and on the Y-axis we have the relative mean metric score. The values are coloured by their metric type in the first graph and their averaging method in the second. The latter has much clearer grouping, indicating the averaging method is very influential and have different levels of optimism/bias. 

It is also worth pointing out that the most conservative metrics appear to be MCC and Cohen Kappa, which naturally generalise to the multiclass setting. 

### Comparison of Metric Type

```{r}

metric_code |> 
  plot_model_metrics(metric_type, 1)

```

### Comparison of Averaging Method

```{r}

metric_code |>
  plot_model_metrics(average_type, 1)

```

## Comparison of Mean and Standard Deviation of Each Metric {.tabset}

---

### Colour by Metric Type

With the relative mean and relative standard deviation on the X and Y axis, respectively, the metrics scores appear to cluster by their metrics, instead of by the model they came from. 

```{r, cache = TRUE}
metric_id |>   
  filter(model_code != "DUM") |> 
  parse_metric_column() |> 
  ggplot(aes(x = rel_mean, y = rel_sd, colour = metric_type, group = metric))+
  geom_point(size = point_size / 3, shape = 15, alpha = .5) +
  geom_hline(yintercept = 1, linewidth = minor_line_width / 3) +
  geom_vline(xintercept = 1, linewidth = minor_line_width / 3) +
  scale_x_continuous("Relative Mean Score") +
  scale_y_continuous("Relatiive SD Score") +
  ggtitle("Comparing Metric Type Across Models") +
  scale_colour_manual("Metric", 
                      values = metric_colours, 
                      labels = metric_labels, 
                      guide = guide_legend(override.aes = list(shape = 15, size = 4, alpha = 1))) +
  report_theme(base_size = 13)
```

### Colour by Averaging Method

It is quite nice to see the clear clusters in the different the averaging methods. This likely indicates this is an important choice because of how different. 

```{r, cache = TRUE}
metric_id |>   
  filter(model_code != "DUM") |> 
  parse_metric_column() |> 
  ggplot(aes(x = rel_mean, y = rel_sd, colour = average_type, group = metric))+
  geom_point(size = point_size / 3, shape = 15, alpha = .5) +
  geom_hline(yintercept = 1, linewidth = minor_line_width / 3) +
  geom_vline(xintercept = 1, linewidth = minor_line_width / 3) +
  scale_x_continuous("Relative Mean Score") +
  scale_y_continuous("Relatiive SD Score") +
  ggtitle("Comparing Averaging Method Across Models") +
  scale_colour_manual("Averaging Method", 
                      values = average_colours, 
                      guide = guide_legend(override.aes = list(shape = 15, size = 4, alpha = 1))) +
  report_theme(base_size = 13)
```

### Colour by Model Type

This was to illustrate the metrics cluster with values of the same metric rather than from the same model. If they clustered by model then comparisons would likely not be possible. 

```{r, cache = TRUE}
metric_id |>   
  filter(model_code != "DUM") |> 
  parse_metric_column() |> 
  ggplot(aes(x = rel_mean, y = rel_sd, colour = model_code, group = metric))+
  geom_point(size = point_size / 3, shape = 15, alpha = .5) +
  geom_hline(yintercept = 1, linewidth = minor_line_width / 3) +
  geom_vline(xintercept = 1, linewidth = minor_line_width / 3) +
  scale_x_continuous("Relative Mean Score") +
  scale_y_continuous("Relatiive SD Score") +
  ggtitle("Cluster of Metrics Across Models") +
  scale_colour_manual("Model", 
                      values = c("GNB" = "#f94144", "GB" = "#f3722c", "KNN" = "#f8961e", 
                                 "LSCM" = "#f9c74f", "LG" = "#90be6d", "RF" = "#43aa8b", 
                                 "SVM" = "#577590"), 
                      guide = guide_legend(override.aes = list(shape = 15, size = 4, alpha = 1))) +
  report_theme(base_size = 13)
```

## Comparison of Models using Dummy Classifer {.tabset}

---

### Metric Scores of Dummy

```{r}
metrics_df |> 
  filter(model_code == "DUM") |> 
  ggplot(aes(x = reorder(metric, score), y = score, fill = average_type)) +
  stat_summary(fun = mean, geom = "bar") +
  scale_x_discrete("Metric", labels = ~ str_replace_all(., "_", " ") |> str_to_title()) +
  scale_y_continuous("Mean Score")+
  scale_fill_manual("Averaging Method", 
                      values = average_colours, 
                      guide = guide_legend(override.aes = list(shape = 15, size = 4, alpha = 1))) +
  coord_flip() +
  ggtitle("Metric Scores from Dummy Classifier") +
  report_theme(15) 
  
```

```{r}

dum_df <- metrics_df |> 
  filter(model_code == "DUM") |> 
  summarise(.by = metric, dum_score = mean(score))


diff_df <- metrics_df |> 
  filter(model_code != "DUM") |> 
  summarise(.by = c(model_code, average_type, metric_type, metric), 
            mean_score = mean(score, na.rm = T)) |> 
  inner_join(dum_df, by = "metric") |> 
  mutate(
    diff = mean_score - dum_score, 
    rel_diff = mean_score / dum_score,
    rel_inc = diff / (1 - dum_score)
    )

```

### Absolute Difference (by average)

The metric score for a model subtract the score from the dummy. 

```{r}

# diff_df |> 
#   plot_folds_average("Micro", rel_inc)

diff_df |> 
  plot_model_metrics(average_type, 
                     y = diff, 
                     y_title = "Absolute Difference from Dummy")

```

### Absolute Difference (by metric)

The metric score for a model subtract the score from the dummy. 

```{r}

# diff_df |> 
#   plot_folds_average("Micro", rel_inc)

diff_df |> 
  plot_model_metrics(metric_type, 
                     y = diff, 
                     y_title = "Absolute Difference from Dummy")

```

### Relative Difference (by average)

The metric score of the model divided by the score from the dummy. A score of 1 is achieved when the model and dummy have the same value. Larger scores indicate larger positive changes in the metric score.

```{r}

diff_df |> 
  plot_model_metrics(average_type, 
                     y = rel_diff, 
                     y_title = "Relative Difference from Dummy")

```

### Relative Difference (by metric)

The metric score of the model divided by the score from the dummy. A score of 1 is achieved when the model and dummy have the same value. Larger scores indicate larger positive changes in the metric score.

```{r}

diff_df |> 
  plot_model_metrics(metric_type, 
                     y = rel_diff, 
                     y_title = "Relative Difference from Dummy")

```

### Relative to Maximum Difference (by average)

The difference between the model and the dummy, divided by one minus the dummy score. For example, if the dummy has a score of 60%, that means the maximum increase that is possible is 40%. So we take the difference and divide by this maximum to see how much the model improves. A score of 1 denotes the model achieved 100% of the possible increase.

```{r}

diff_df |> 
  plot_model_metrics(average_type, 
                     y = rel_inc, 
                     y_title = "Relative Difference from Maximum Increase")

```

### Relative to Maximum Difference (by metric)

The difference between the model and the dummy, divided by one minus the dummy score. For example, if the dummy has a score of 60%, that means the maximum increase that is possible is 40%. So we take the difference and divide by this maximum to see how much the model improves. A score of 1 denotes the model achieved 100% of the possible increase.

```{r}

diff_df |> 
  plot_model_metrics(metric_type, 
                     y = rel_inc, 
                     y_title = "Relative Difference from Maximum Increase")

```


## Comparison of Models Using Metrics* {.tabset}

---

> *Note: these graphs are not in their final form

The five folds used in the cross validation are the same across all the models, meaning a comparison is more appropriate compared to if the folds were completely random. It is clear the best type of model is the Gradient Boosted Tree whilst Gaussian Naive Bayes appears the weakest. This could be partially explained by Gradient Boosting having the most number of individual models, due to having several hyperparameters. The hyperparameter tuning of these models have yet to be finished. Currently, a wide range of hyperparameters have been explored but a more narrow and focused scope could improve model performance. 

### Micro Averaging

```{r}
metrics_df |> 
  plot_folds_average("Micro")
```


### Macro Averaging

```{r}
metrics_df |>
  plot_folds_average("Macro")

```

### Weighted Averaging

```{r}
metrics_df |>
  plot_folds_average("Weighted")

```

### No Averaging

```{r}

metrics_df |>
  plot_folds_average("None")

```

## Comparison of Top Model Using Metrics

---

```{r}

top_metrics_df <- metrics_df |> 
  filter(model_code != "DUM") |> 
  summarise(.by = c(metric, model_id, model_code), 
            mean_score = mean(score)) |> 
  slice_max(order_by = mean_score, 
            by = c(model_code, metric), 
            n = 10) |> 
  left_join(metrics_df, by = c("metric", "model_id", "model_code"))

top_metrics_df_2 <- top_metrics_df|> 
  select(-score, -id, -fold) |> 
  distinct()


```

### Micro Averaging

```{r}
# top_metrics_df |> 
#   plot_model_metrics(colour = average_type, 
#                      y = mean_score, 
#                      y_title = "Mean Score")
#                      
# top_metrics_df |> 
  # ggplot(aes(x = model_code, 
  #              y = score, 
  #              colour = average_type))+
  # # geom_violin(alpha = .4)+
  # geom_jitter(size = point_size, shape = 15, alpha = .8)+
  # scale_x_discrete("Model Type") +
  # scale_y_continuous("Mean Score") +
  # ggtitle("Comparing Averaging Methods Across Models") +
  # scale_colour_manual("Averaging Method",
  #                     values = average_colours,
  #                     guide = guide_legend(override.aes = list(shape = 15, size = 4, alpha = 1)))+
  # report_theme(base_size = 13)
  #
```


```{r}

top_metrics_df |>
  plot_folds_func("Micro", func = "max")

```


### Macro Averaging

```{r}

top_metrics_df |>
  plot_folds_func("Macro", func = "max")

```

### Weighted Averaging

```{r}

top_metrics_df |>
  plot_folds_func("Weighted", func = "max")

```

### No Averaging

```{r}

top_metrics_df |>
  plot_folds_func("None", func = "max")

```

### Testing Violin Plots

```{r}

top_metrics_df_2 |> 
  filter(average_type == "None") |> 
  ggplot(aes(x = reorder(model_code, mean_score), 
             y = mean_score, 
             fill = model_code, 
             colour = model_code))+
  # geom_violin()+
  geom_jitter() +
  facet_wrap(~metric_type, nrow = 2)

```


### Testing Heatmap

```{r}

top_metrics_df_2 |> 
  summarise(.by = c(metric, model_code), 
            max_score = max(mean_score)) |> 
  ggplot(aes(x = reorder(model_code, max_score), y = reorder(metric, max_score), fill = max_score))+
  geom_tile(colour = "white") +
  geom_text(aes(label = round(max_score, 2)))+
  scale_fill_gradientn("Corr",
                       colours = c("blue", "grey80", "red"))


```


## Correlation of Metrics {.tabset}

---

### Pearson's r

```{r}
library(ggcorrplot)

corr_pear <- metrics |> 
  select(where(is.numeric)) |> 
  rename_with( ~ str_replace_all(., "_", " ") |> str_to_title()) |>
  scale() |>
  cor(use = "complete.obs", 
      method = "pearson")

ggcorrplot(corr_pear, 
           lab = T,
           type = "lower", 
           title = "Pearson Correlation Between All the Metrics")+
  scale_fill_gradientn("Corr",
                       colours = c("blue", "red"),
                       limit = c(.65, 1))

```

### Spearman's r

```{r}
corr_spear <- metrics |> 
  select(where(is.numeric)) |> 
  rename_with( ~ str_replace_all(., "_", " ") |> str_to_title()) |>
  scale() |>
  cor(use = "complete.obs", 
      method = "spearman")

ggcorrplot(corr_spear, 
           lab = T,
           type = "lower", 
           title = "Spearman Correlation Between All the Metrics")+
  scale_fill_gradientn("Corr",
                       colours = c("blue", "red"),
                       limit = c(.65, 1))
```

### Pearson's r (top models)

```{r}
top_ids <- top_metrics_df |> 
  pluck("model_id")

top_corr_pear <- metrics |> 
  filter(model_id %in% top_ids) |> 
  select(where(is.numeric)) |> 
  rename_with( ~ str_replace_all(., "_", " ") |> str_to_title()) |>
  scale() |>
  cor(use = "complete.obs", 
      method = "pearson")

ggcorrplot(top_corr_pear, 
           lab = T,
           type = "lower", 
           title = "Spearman Correlation Between All the Metrics")+
  scale_fill_gradientn("Corr",
                       colours = c("blue", "red"),
                       limit = c(.65, 1))
```

### Spearmans's r (top models)

```{r}
top_ids <- top_metrics_df |> 
  pluck("model_id")

top_corr_spear <- metrics |> 
  filter(model_id %in% top_ids) |> 
  select(where(is.numeric)) |> 
  rename_with( ~ str_replace_all(., "_", " ") |> str_to_title()) |>
  scale() |>
  cor(use = "complete.obs", 
      method = "spearman")

ggcorrplot(top_corr_spear, 
           lab = T,
           type = "lower", 
           title = "Spearman Correlation Between All the Metrics")+
  scale_fill_gradientn("Corr",
                       colours = c("blue", "red"),
                       limit = c(.65, 1))
```

## Confusion Matrix

---

### Testing Boxplots for the Values from a Confusion Matrix 

```{r}

conf_df <- confusion |> 
  filter(model_id %in% top_ids) |> 
  pivot_longer(
    cols = starts_with(c("tp_", "fp_", "tn_", "fn_")),
    names_to = c("metric", "group"), 
    values_to = "value", 
    names_sep = "_"
    )

```


```{r}

conf_df |> 
  ggplot(aes(x = model_code, y = value, fill = group))+
  geom_boxplot(alpha = .8)+
  facet_wrap(~metric, nrow = 2, scales = "free_y")

```

### Testiing Heatmaps for the Values from the Confusion Matrix

```{r}

conf_df |> 
  mutate(
    group = factor(group, levels = sort(unique(conf_df$group)), ordered = T), 
    metric_group = str_c(metric, "_", group), 
         ) |> 
  arrange(group) |> 
  ggplot(aes(x = model_code, y = metric_group, fill = value))+
  geom_tile(colour = "grey70")

```


## Comparing the Ranks of the Best Models

```{r}

top_metrics_df_2 |> 
  summarise(.by = c(model_code, metric), rank = rank(mean_score, ties.method = "max")) |> 
  filter(model_code == "GB") |> 
  ggplot(aes(x = reorder(metric, rank), y = rank, colour = model_code, group = model_code))+
  geom_line()

```

## Comparison of Metrics, with an Individual Metric on x-axis

```{r}
top_metrics_df_2 |> 
  pluck("model_id")

ba_df <- metric_id |> 
  filter(model_id %in% pluck(top_metrics_df_2, "model_id")) |> 
  filter(metric == "balanced_accuracy") |> 
  mutate(bal_acc = rel_mean) |> 
  select(model_id, bal_acc)

metric_id |> 
  filter(model_id %in% pluck(top_metrics_df_2, "model_id")) |> 
  filter(metric != "balanced_accuracy") |> 
  inner_join(ba_df, by = "model_id") |> 
  parse_metric_column() |> 
  ggplot(aes(x = bal_acc, y = rel_mean, colour = average_type))+
  geom_point() +
  geom_hline(aes(yintercept = 1)) +
  geom_smooth(method = "lm", se = F)



```


