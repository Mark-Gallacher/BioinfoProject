---
title: "Hyperparameter Tuning"
author: '2466057'
date: "2024-05-17"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    css: "style.css"
    df_print: "paged"
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = TRUE, 
                      fig.width = 10, 
                      fig.height = 8,
                      fig.path = "./figures/hyperparams/", 
                      comment = "")

```

```{r}

library(tidyverse)
library(here)

source(here("r/setup_subtypes.R"))

theme_set(report_theme())

```

### Overview

---

This covers the basic hyperparameter tuning of the models. In the cases where there are several hyperparameters, like for Random Forest and Gradient Boosting Tree, further tuning is required to ensure we have found the near optimal hyperparameters. It would not be computationally sensible to exhaustively search the hyperparameter space of the ensemble techniques. Instead the tuning could be done in two phases. The first phase would be more general and broad while the final phase would be search a finer scope of values, using the insights from the previous phase. This document covers the final phase. 

```{r}

metrics |> 
  filter(!model_code %in% c("DUM", "GNB", "ERF")) |>
  count(model_code, sort = TRUE) |>   
  ggplot(aes(x = reorder(model_code, n), y = n / 5 , fill = model_code)) +
  geom_col(show.legend = F, colour = "black", alpha = .8) +
  coord_flip() +
  scale_x_discrete("Model") +
  scale_y_continuous("Number of Models") +
  ggtitle("Number of Models in Hyperparameter Tuning") +
  scale_fill_manual("Model", 
                    values = c("GNB" = "#f94144", "GB" = "#f3722c", "KNN" = "#f8961e", 
                               "LSVM" = "#f9c74f", "LG" = "#90be6d", "RF" = "#43aa8b", 
                               "ERF" = "darkgreen", "SVM" = "#577590"), 
                    guide = guide_legend(override.aes = list(shape = 15, size = 4, alpha = 1)))

```

```{r}
.num_models <- metrics |> 
  filter(!model_code %in% c("DUM", "GNB", "ERF")) |>
  count(model_code, sort = TRUE) |> 
  mutate(n = n / 5) |> 
  mutate(`Proportion (%)` = round(100 * (n / sum(n)), 2)) |> 
  rename("Model Type" = model_code) |> 
  rename("Number of Models" = n) 

.num_models
```

```{r}
.num_models |> 
  summarise(Total = sum(`Number of Models`))
```


```{r}
for (name in names(all_names)){
  cat(paste0(name, " - ", all_names[name], "\n"))
}
```


## Logistic Regression {.tabset}

---

Logistic Regression (LG) was trained across three different hyperparemeters, each relating to regularisation. 

 - **C**, is the strength of regularisation, with larger values indicating a smaller regularisation effect. 
 - **Penalty** refers to the four type of regularisation tested, from None (No regularisation), L1, L2 and ElasticNet (L1 + L2). 
 - **L1 ratio** ratio between L1 and L2 (only applicable for ElasticNet)

### C

```{r}
plot_hyperparam_1n("LG", C) +
  scale_x_log10()
```

### Penalty

```{r}
plot_hyperparam_1c("LG", penalty)
```

### L1 Ratio

```{r}
plot_hyperparam_1n("LG", l1_ratio)

```

### Best Hyperparameters for each Metric

```{r}

get_summary_from_code("LG")|> 
  slice_max(by = metric, n = 1, order_by = mean_score)|>
  rename_with(~str_to_title(str_replace_all(.x, "_", " ")))

```


## K-Nearest Neighbours

---

K-Nearest Neighbours (KNN) was trained across only two hyperparameters:

  - **K** (number of neighbours) the number of nieghbouring points involved in the calculation
  - **Weight**, how the proxomity of these neighbouring points are weighted (Uniform or proportional to Distance). 

```{r}

plot_hyperparam_1n_1c("KNN", n_neighbors, weights)

```

### Best Hyperparameters for each Metric

```{r}

get_summary_from_code("KNN")|> 
  slice_max(by = metric, n = 1, order_by = mean_score)|>
  rename_with(~str_to_title(str_replace_all(.x, "_", " ")))

```


## Support Vector Machine {.tabset}

---

A variety of Support Vector Machines (SVM) were trained on three hyperparemeters:

  - **C**, like in logistic regression, this is the regularisation factor (smaller values means larger penalty)
  - **Kernel**, the core of the SVM either Linear, Polynomial or a Radial Basis Function (RBF)
  - **gamma**, determines the influence of a single data point. Larger values mean the points need to be closer to influence each other.
  - **Class Weight**, the weight for the individual C values - one for each class. Default is 1, "Balanced" is inversely proportional to class frequency. 
  
Note: Gamma only applies to a polynomial or RBF kernel, and scikit-learn offers two methods of determining the appropriate value through "Auto" and "Scale" - the maths behind the scenes should be described in the future.  

### C

```{r}
plot_hyperparam_1n("SVM", C) + scale_x_log10()
```

### Kernel

```{r}
plot_hyperparam_1c("SVM", kernel)
```


### Gamma

```{r}
plot_hyperparam_1c("SVM", gamma)
```

### Class Weight

```{r}
plot_hyperparam_1c("SVM", class_weight)
```

### Best Hyperparameters for Each Metric

```{r}

get_summary_from_code("SVM")|> 
  slice_max(by = metric, n = 1, order_by = mean_score)|>
  rename_with(~str_to_title(str_replace_all(.x, "_", " ")))

```

## Linear Support Vector Machine* {.tabset}

---

> *Scikit-learn has two APIs for a SVM, the latter only supports a linear kernel but offers more methods of regularisation. It is also reported to be significantly quicker - by an order of magnitude - for larger datasets. 

A Linear SVM (LSVM) does not take in hyperparameters kernel or gamma, since it is constrained to a linear kernel which does not have a gamma parameter. However, due to various technical issues, the penalty used was only L2 instead of including L1 and ElasticNet. The hyperparameters used were:

  - **C**, the scale of regularisation, larger values indicate smaller penalties.
  - **Loss**, how an error is determined, Hinge or Squared Hinge.
  - **Class Weight**, the weight for the individual C values - one for each class. Default is 1, "Balanced" is inversely proportional to class frequency. 

### C

```{r}
plot_hyperparam_1n("LSVM", C) + scale_x_log10()
```

### Loss

```{r}
plot_hyperparam_1c("LSVM", loss)

```

### Best Hyperparameter for Each Metric

```{r}

get_summary_from_code("LSVM")|> 
  slice_max(by = metric, n = 1, order_by = mean_score)|>
  rename_with(~str_to_title(str_replace_all(.x, "_", " ")))

```

## Random Forest {.tabset}

---

Random Forest (RF) is an ensembl technique trains several decision trees and aggregates across them to form a stronger predictor. RF has several hyperparameters to test, not all the parameters were selected as they are not all equally important. The selected few were:

  - **Number of trees**, this is simply the number of trees trained. 
  - **Max Depth**, a common parameter to regularise a decision tree, to control how many layers it travels down before terminating.
  - **Minimum Samples in a Split**, the smaller number of samples allowed when splitting a node. 
  - **Minimum Samples for a leaf node**, the minimum allowed number of samples before a leaf node is created.
  - **Max Number of Features**, determines how many features to use
  - **Criteron**, Gini impurity or Shannon Entropy. Determines the best feature(s) to split the node. 
  - **Class Weight**, the weight for the individual C values - one for each class. Default is 1, "Balanced" is inversely proportional to class frequency. 


### Number of Trees

```{r}
plot_hyperparam_1n("RF", n_estimators)
```

### Max Depth

```{r}
plot_hyperparam_1n("RF", max_depth)
```

### Min Samples Splits

```{r}
plot_hyperparam_1n("RF", min_samples_split)
```

### Min Samples Leaf

```{r}
plot_hyperparam_1n("RF", min_samples_leaf)
```

### Max Number of Features

```{r}
plot_hyperparam_1c("RF", max_features)
```

### Criterion

```{r}
plot_hyperparam_1c("RF", criterion)
```

### Class Weight

```{r}
plot_hyperparam_1c("RF", class_weight)
```

### Best Hyperparameters for Each Metric

```{r}

get_summary_from_code("RF")|> 
  slice_max(by = metric, n = 1, order_by = mean_score)|>
  rename_with(~str_to_title(str_replace_all(.x, "_", " ")))

```

## ExtraRandom Forest {.tabset}

---

Note: This model is not presented in the final report due to the similarity with Random Forest. Sometimes with a lot of features the additional randomness leads to better models. 

ExtraRandom Forest (ERF) is a variation of Random Forests, where the features for the splits are randomly selected, instead of determining the most discriminating features. The hyperparameters are very similar :

  - **Number of trees**, this is simply the number of trees trained. 
  - **Max Depth**, a common parameter to regularise a decision tree, to control how many layers it travels down before terminating.
  - **Minimum Samples in a Split**, the smaller number of samples allowed when splitting a node. 
  - **Minimum Samples for a leaf node**, the minimum allowed number of samples before a leaf node is created.
  - **Max Number of Features**, determines how many features to use
  - **Criteron**, Gini impurity or Shannon Entropy. Determines the best feature(s) to split the node. 
  - **Class Weight**, the weight for the individual C values - one for each class. Default is 1, "Balanced" is inversely proportional to class frequency. 


### Number of Trees

```{r}
plot_hyperparam_1n("ERF", n_estimators)
```

### Max Depth

```{r}
plot_hyperparam_1n("ERF", max_depth)
```

### Min Samples Splits

```{r}
plot_hyperparam_1n("ERF", min_samples_split)
```

### Min Samples Leaf

```{r}
plot_hyperparam_1n("ERF", min_samples_leaf)
```

### Max Number of Features

```{r}
plot_hyperparam_1c("ERF", max_features)
```

### Criterion

```{r}
plot_hyperparam_1c("ERF", criterion)
```

### Class Weight

```{r}
plot_hyperparam_1c("ERF", class_weight)
```

### Best Hyperparameters for Each Metric

```{r}

get_summary_from_code("ERF")|> 
  slice_max(by = metric, n = 1, order_by = mean_score) |>
  rename_with(~str_to_title(str_replace_all(.x, "_", " ")))

```

## Gradient Boosted Tree {.tabset}

---

Similar to Random Forest, Gradient Boosted Trees (GB) have several hyperparameters to tune. However, the same parameters for the RF were used here. GB also has an additional hyperparameter that determines how the previous generation of trees influence the current tree (i.e the learning rate). 

  - **Number of trees** (n_estimators), this is simply the number of trees trained. 
  - **Max Depth**, a common parameter to regularise a decision tree, to control how many layers it travels down before terminating.
  - **Minimum Samples allowed in a Split**, the smaller number of samples allowed when splitting a node. 
  - **Minimum Samples for a leaf node**, the minimum allowed number of samples before a leaf node is created.
  - **Max Number of Features**, determines how many features to use
  - **Learning Rate**, larger values indicate a greater influence from past tree - quicker learning at the cost of decreased flexibility

### Number of Trees

```{r}
plot_hyperparam_1n("GB", n_estimators)
```

### Max Depth

```{r}
plot_hyperparam_1n("GB", max_depth)
```

### Min Samples Split

```{r}
plot_hyperparam_1n("GB", min_samples_split)
```

### Min Samples Leaf

```{r}
plot_hyperparam_1n("GB", min_samples_leaf)
```

### Max number of Features

```{r}
plot_hyperparam_1c("GB", max_features)
```

### Learning Rate

```{r}
plot_hyperparam_1n("GB", learning_rate)
```

### Best Hyperparameters for Each Metric

```{r}

get_summary_from_code("GB") |> 
  slice_max(by = metric, n = 1, order_by = mean_score) |> 
  rename_with(~str_to_title(str_replace_all(.x, "_", " ")))
  

```
